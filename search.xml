<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>【转载】基于深度学习的目标检测发展综述</title>
      <link href="/main/2018/12/30/example/1/"/>
      <url>/main/2018/12/30/example/1/</url>
      
        <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0 前言"></a>0 前言</h2><p>  所谓<strong>目标检测</strong>，就是找出图像中所有感兴趣的目标（物体），确定它们的位置和大小，是机器视觉领域的核心问题之一。由于各类物体有不同的<strong>外观、形状、姿态</strong>，加上成像时<strong>光照、角度、遮挡</strong>等因素的干扰，目标检测一直是机器视觉领域最具有挑战性的问题之一。</p><a id="more"></a><p>计算机视觉中关于图像识别有四大类任务：</p><p><strong>分类(Classification)</strong>：解决“是什么”的问题，即给定一张图片或一段视频判断里面包含什么类别的目标；</p><p><strong>定位(Location)</strong>：解决“在哪里”的问题，即定位出这个目标的的位置，通常为但目标问题；</p><p><strong>检测(Detection)</strong>：解决“是什么、在哪里”的问题，即定位出这个目标的的位置并且知道目标物分别是什么，通常为多目标问题；</p><p><strong>分割(Segmentation)</strong>：分为实例的分割（Instance-level）和场景分割（Scene-level），解决“每一个像素属于哪个目标物或场景”的问题；</p><p><img src="https://img.vim-cn.com/1b/dab7ab30214ec750a0442b4083f609c7424321.webp" alt="目标检测任务分类" title="目标检测任务分类"></p><center><font face="黑体">图0.1:目标检测任务分类</font></center><p>目标检测对于人类来说并不困难，通过对图片中不同颜色模块的感知很容易定位并分类出其中目标物体，但对于计算机来说，面对的是RGB像素矩阵，很难从图像中直接得到狗和猫这样的抽象概念并定位其位置，再加上有时候多个物体和杂乱的背景混杂在一起，目标检测变得十分困难。</p><p>但科学技术永远是第一生产力，目标检测始终是科研人员研究的热点问题，各种理论和方法不断涌现。</p><p>在传统视觉领域，目标检测主要集中于一些特定目标的检测，比如人脸检测和行人检测等，已经形成了非常成熟的技术。但对于其他普通的目标检测虽有过很多的尝试，但是效果总是不太令人满意。</p><p>传统的目标检测一般使用<strong>滑动窗口的框架</strong>，主要包括三个步骤：</p><p><strong>1)利用不同尺寸的滑动窗口框住图中的某一部分作为候选区域；</strong></p><p><strong>2)提取候选区域相关的视觉特征。</strong>比如人脸检测常用的Harr特征、行人检测和普通目标检测常用的HOG特征等；</p><p><strong>3)利用分类器进行识别，</strong>如常用的 SVM 模型。</p><p>传统的目标检测中，多尺度形变部件模型DPM (Deformable Part Model)是出类拔萃的，连续获得VOC (Visual Object Class) 2007—2009的检测冠军，2010年其作者Felzenszwalb Pedro被VOC授予”终身成就奖”。DPM把物体看成了多个组成的部件（比如人脸的鼻子、嘴巴等），用部件间的关系来描述物体，这个特性非常符合自然界很多物体的非刚体特征。DPM可以看做是 HOG+SVM 的扩展，很好的继承了两者的优点，在人脸检测、行人检测等任务上取得了不错的效果，但是DPM相对复杂，检测速度也较慢，从而也出现了很多改进的方法。正当大家热火朝天改进DPM性能的时候，基于深度学习的目标检测横空出世，迅速盖过了DPM的风头，很多之前研究传统目标检测算法的研究者也开始转向深度学习。</p><p>基于深度学习的目标检测发展起来后，其实效果也一直难以突破。比如文献“Deep Neural Networks for Object Detection. Advances in Neural Information Processing Systems 26 (NIPS), 2013”中的算法在VOC 2007测试集合上的mAP只能30%多一点，文献“OverFeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014”中的OverFeat在ILSVRC 2013测试集上的mAP只能达到24.3%。2013年R-CNN诞生了，VOC 2007测试集的mAP被提升至48%，2014年时通过修改网络结构又飙升到了66%，同时ILSVRC 2013测试集的mAP也被提升至31.4%。</p><p>R-CNN，即Region-based Convolutional Neural Networks，是一种结合区域提名(Region Proposal) 和卷积神经网络(CNN)的目标检测方法。Ross Girshick在2013年的开山之作《Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation》奠定了这个子领域的基础。</p><p>R-CNN这个领域目前研究非常活跃，先后出现了R-CNN、SPP-net、Fast R-CNN、Faster R-CNN、R-FCN、YOLO、SSD等研究。Ross Girshick作为这个领域的开山鼻祖，在这发展过程中做出了巨大的贡献，R-CNN、Fast R-CNN、Faster R-CNN、YOLO都和他有关。这些创新的工作其实很多时候是把一些传统视觉领域的方法和深度学习结合起来了，比如选择性搜索（Selective Search)和图像金字塔（Pyramid）等。</p><p>深度学习相关的目标检测方法目前大致分为两派：</p><p><strong>1)基于区域提名的</strong>，如R-CNN、SPP-net、Fast R-CNN、Faster R-CNN、R-FCN等；<br><strong>2)端到端（End-to-End）</strong>，无需区域提名的，如YOLO、SSD等。</p><p>目前来说，基于区域提名的方法占据上风，但端到端的方法速度上优势明显，后续的发展潜力较大。</p><center><br><img src="https://img.vim-cn.com/fc/3323f987f0b0b3d642cc54b3055acacc0b2724.webp" width="100%"><br><center><font face="黑体">图0.2:目标检测方法概述</font></center><h2 id="1-相关研究"><a href="#1-相关研究" class="headerlink" title="1 相关研究"></a>1 相关研究</h2><p>我们先来回顾一下目标检测中广泛使用的区域提名——选择性搜索，以及用深度学习做目标检测的早期工作——Overfeat 。</p><h3 id="1-1-选择性搜索"><a href="#1-1-选择性搜索" class="headerlink" title="1.1 选择性搜索"></a>1.1 选择性搜索</h3><p>目标检测的第一步是要做区域提名(Region Proposal)，也就是找出可能的感兴趣区域（Region Of Interest, ROI）。区域提名类似于光学字符识别 (OCR) 领域的切分，OCR切分常用过切分方法，简单说就是尽量切碎到小的连通域（比如小的笔画之类），然后再根据相邻块的一些形态学特征进行合并。但目标检测的对象相比OCR领域千差万别，而且图形不规则，大小不一，所以一定程度上可以说区域提名是比OCR切分更难的一个问题。</p><p>区域提名可能的方法有：</p><p><strong>1)滑动窗口</strong>。滑动窗口本质上是穷举法，利用不同的尺度和长宽比把所有可能的大大小小的块都穷举出来，然后送去识别，识别出来概率大的就留下来。显然，这样的方法复杂度太高，产生了很多的冗余候选区域，在现实当中不可行。</p><p><strong>2)规则块</strong>。在穷举法的基础上进行了一些剪枝，只选用固定的大小和长宽比。这在一些特定的应用场景中是很有效的，比如拍照搜题APP小猿搜题中的汉字检测，因为汉字方方正正，长宽比大多比较一致，因此用规则块做区域提名是一种比较合适的选择。但是对于普通的目标检测来说，规则块依然需要访问很多的位置，复杂度高。</p><p><strong>3)选择性搜索</strong>。从机器学习的角度来说，前面的方法召回是不错了，但是精度并不能令人满意，问题的核心在于如何有效地去除冗余候选区域。其实冗余候选区域大多是发生了重叠，选择性搜索利用这一点，自底向上合并相邻的重叠区域，从而减少冗余。</p><p>当然，区域提名并不只有以上所说的三种方法，实际上这一方面是非常灵活的，因此变种也很多，感兴趣的读者自行参考相关文献。</p><p>选择性搜索的具体算法细节如下所示。总体上选择性搜索是自底向上不断合并候选区域的迭代过程。</p><p><img src="https://img.vim-cn.com/23/e47c866a6cd07a84568cdc873314643b899d47.webp" alt=""></p><p>输入: 一张图片<br>输出：候选的目标位置集合L</p><p><strong>算法：</strong><br>1)利用过切分方法得到候选的区域集合R = {r1,r2,…,rn}<br>2)初始化相似集合S = ϕ<br>3) foreach 相邻区域对(ri,rj) do</p><center>计算相似度s(ri,rj)<br><br>S = S ∪ s(ri,rj)</center><p>4)while S not=ϕ do</p><center>得到最大的相似度s(ri,rj)=max(S)<br><br>合并对应的区域rt = ri ∪ rj<br><br>移除ri对应的所有相似度：S = S\s(ri,r<em>)<br><br>移除rj对应的所有相似度：S = S\s(r</em>,rj)<br><br>计算rt对应的相似度集合St<br><br>S = S ∪ St<br><br>R = R ∪ rt </center><p>5)L = R中所有区域对应的边框</p><p>从算法不难看出，R中的区域都是合并后的，因此减少了不少冗余，相当于准确率提升了，但是我们还需要继续保证召回率，因此选择性搜索算法中的相似度计算策略就显得非常关键了。如果简单采用一种策略很容易错误合并不相似的区域，比如只考虑轮廓时，不同颜色的区域很容易被误合并。选择性搜索采用多样性策略来增加候选区域以保证召回，比如颜色空间考虑RGB、灰度、HSV及其变种等，相似度计算时既考虑颜色相似度，又考虑纹理、大小、重叠情况等。</p><p>总体上，选择性搜索是一种比较朴素的区域提名方法，被早期的基于深度学习的目标检测方法（包括Overfeat 和R-CNN等）广泛采用，但被当前的新方法弃用了。</p><h3 id="1-2-OverFeat"><a href="#1-2-OverFeat" class="headerlink" title="1.2 OverFeat"></a>1.2 OverFeat</h3><p>OverFeat[论文下载链接]是用CNN统一来做分类、定位和检测的经典之作，作者是深度学习大神Yann Lecun在纽约大学的团队。OverFeat也是ILSVRC 2013任务3（分类+定位）的冠军得主。</p><p>OverFeat的核心思想有三点：</p><p><strong>1)区域提名：</strong>结合滑动窗口和规则块，即多尺度（multi-scale)的滑动窗口；</p><p><strong>2)分类和定位：</strong>统一用CNN来做分类和预测边框位置，模型与AlexNet类似，其中1-5层为特征抽取层，即将图片转换为固定维度的特征向量，6-9层为分类层(分类任务专用)，不同的任务（分类、定位、检测）共用特征抽取层（1-5层），只替换6-9层；</p><p><strong>3)累积：</strong>因为用了滑动窗口，同一个目标对象会有多个位置，也就是多个视角；因为用了多尺度，同一个目标对象又会有多个大小不一的块。这些不同位置和不同大小块上的分类置信度会进行累加，从而使得判定更为准确。</p><p>OverFeat的关键步骤有四步：</p><p><strong>1)利用滑动窗口进行不同尺度的区域提名，然后使用CNN模型对每个区域进行分类，得到类别和置信度。</strong>从下图可以看出，不同缩放比例时，检测出来的目标对象数量和种类存在较大差异</p><p><img src="https://img.vim-cn.com/ee/fb48f2e998a014e2bee97a5f341dad9bc79d96.webp" alt="图1.1.1: Overfeat关键步骤一" title="Overfeat关键步骤一"></p><center><font face="黑体">图1.1.1: Overfeat关键步骤一<font></font></font></center><p><strong>2)利用多尺度滑动窗口来增加检测数量，提升分类效果</strong>，如下图所示</p><p><img src="https://img.vim-cn.com/63/2dbbc63d10daa8da6d7cb19cdc58702c5d60e3.webp" alt="图1.1.2: Overfeat关键步骤二" title="Overfeat关键步骤二"></p><center><font face="黑体">图1.1.2: Overfeat关键步骤二<font></font></font></center><p><strong>3)用回归模型预测每个对象的位置</strong>，从下图来看，放大比例较大的图片，边框数量也较多</p><p><img src="https://img.vim-cn.com/d9/4406003d645ac4cce97ad5f2013bf88e19098b.webp" alt="图1.1.3: Overfeat关键步骤三" title="Overfeat关键步骤三"></p><center><font face="黑体">图1.1.3: Overfeat关键步骤三<font></font></font></center><p><strong>4)边框合并</strong></p><p><img src="https://img.vim-cn.com/ef/6ba2287ff6fd8337fb60fda6ec39b8ebda118c.webp" alt="图1.1.4: Overfeat关键步骤四" title="Overfeat关键步骤四"></p><center><font face="黑体">图1.1.4: Overfeat关键步骤四<font></font></font></center><p>Overfeat是CNN用来做目标检测的早期工作，主要思想是采用了多尺度滑动窗口来做分类、定位和检测，虽然是多个任务但重用了模型前几层，这种模型重用的思路也是后来R-CNN系列不断沿用和改进的经典方法。</p><p>当然Overfeat也是有不少缺点的，至少速度和效果都有很大提升空间，后面的R-CNN系列在这两方面做了很多提升。</p><h2 id="2-基于区域提名的方法"><a href="#2-基于区域提名的方法" class="headerlink" title="2 基于区域提名的方法"></a>2 基于区域提名的方法</h2><p>本小节主要介绍基于区域提名的方法，包括R-CNN、SPP-net、Fast R-CNN、Faster R-CNN、R-FCN。</p><h3 id="2-1-R-CNN"><a href="#2-1-R-CNN" class="headerlink" title="2.1 R-CNN"></a>2.1 R-CNN</h3><p>如前面所述，早期的目标检测，大都使用滑动窗口的方式进行窗口提名，这种方式本质是穷举法，R-CNN[论文下载链接]采用的是Selective Search。</p><p>以下是R-CNN的主要步骤：</p><p><strong>1)区域提名</strong>：通过Selective Search从原始图片提取2000个左右区域候选框；</p><p><strong>2)区域大小归一化</strong>：把所有侯选框缩放成固定大小（原文采用227×227）；</p><p><strong>3)特征提取</strong>：通过CNN网络，提取特征；</p><p><strong>4)分类与回归</strong>：在特征层的基础上添加两个全连接层，再用SVM分类来做识别，用线性回归来微调边框位置与大小，其中每个类别单独训练一个边框回归器。</p><p>其中目标检测系统的结构如下图所示，注意，图中的第2步对应步骤中的1、2步，即包括区域提名和区域大小归一化。</p><p><img src="https://img.vim-cn.com/4e/5543bdb197e7179be44b0616252825ae30fd40.webp" alt="图1.2.1: R-CNN框架" title="R-CNN框架"></p><center><font face="黑体">图1.2.1: R-CNN框架<font></font></font></center><p>OverFeat可以看做是R-CNN的一种特殊情况，只需要把Selective Search换成多尺度的滑动窗口，每个类别的边框回归器换成统一的边框回归器，SVM换为多层网络即可。但是OverFeat实际比R-CNN快9倍，这主要得益于卷积相关的共享计算。</p><p>事实上，R-CNN有很多缺点：</p><p><strong>1)重复计算</strong>：R-CNN虽然不再是穷举，但依然有2000个左右的候选框，这些候选框都需要进行CNN操作，计算量依然很大，其中有不少其实是重复计算；</p><p><strong>2)SVM模型</strong>：而且还是线性模型，在标注数据不缺的时候显然不是最好的选择；</p><p><strong>3)训练测试分为多步</strong>，训练的空间和时间代价很高：区域提名、特征提取、分类、回归都是断开的训练的过程，中间数据还需要单独保存，卷积出来的特征需要先存在硬盘上，这些特征需要几百G的存储空间；</p><p><strong>4)速度慢</strong>：前面的缺点最终导致R-CNN出奇的慢，GPU上处理一张图片需要13秒，CPU上则需要53秒。</p><p>看到这里，肯定不少小伙伴会产生疑惑，既然R-CNN速度很慢，那为什么还说它是改进版？事实上，R-CNN的改进体现在其效果上的提升，其中ILSVRC 2013数据集上的mAP由OverFeat的24.3%提升到了31.4%，第一次有了质的改变。</p><h3 id="2-2-SPP-net"><a href="#2-2-SPP-net" class="headerlink" title="2.2 SPP-net"></a>2.2 SPP-net</h3><p>SPP-net<a href="https://arxiv.org/abs/1406.4729" target="_blank" rel="noopener">论文下载链接</a>是MSRA何恺明等人提出的，其主要思想是去掉了原始图像上的crop/warp等操作，换成了在卷积特征上的空间金字塔池化层（Spatial Pyramid Pooling，SPP），如下图所示。之所以要引入SPP层 ，主要原因是CNN的全连接层要求输入图片大小一致，而实际中的输入图片往往大小不一，如果直接缩放到同一尺寸，很可能有的物体会充满整个图片，而有的物体可能只能占到图片的一角。</p><p>传统的解决方案是进行不同位置的裁剪，但是这些裁剪技术往往会导致一些问题出现，比如下图的crop会导致物体不全，warp导致物体被拉伸后形变严重，SPP就是为了解决这种问题的。SPP对整图提取固定维度的特征，再把图片均分成4份，每份提取相同维度的特征，再把图片均分为16份，以此类推。可以看出，无论图片大小如何，提取出来的维度数据都是一致的，这样就可以统一送至全连接层了。SPP思想在后来的R-CNN模型中也被广泛用到。</p><p><img src="https://img.vim-cn.com/58/fe42ebe2c2007f81078a8311e8e2ff3c5d83d3.webp" alt="图1.2.2: 传统crop/warp结构和空间金字塔池化网络的对比" title="传统crop/warp结构和空间金字塔池化网络的对比"></p><center><font face="黑体">图1.2.2: 传统crop/warp结构和空间金字塔池化网络的对比<font></font></font></center><p>SPP-net的网络结构如下图所示，实质是最后一层卷积层后加了一个SPP层，将维度不一的卷积特征转换为维度一致的全连接输入。</p><p><img src="https://img.vim-cn.com/ac/5550f59aab041a38fb42dc3e8769d42deafbd6.png" alt="图1.2.3: SPP-net网络结构" title="SPP-net网络结构"></p><p><center><strong><font face="黑体">图1.2.3: SPP-net网络结构<font></font></font></strong></center></p><p>SPP-net做目标检测的主要步骤为：</p><p><strong>1)区域提名</strong>：用Selective Search从原图中生成2000个左右的候选窗口；<br><strong>2)区域大小缩放</strong>：SPP-net不再做区域大小归一化，而是缩放到min(w, h)=s，即统一长宽的最短边长度，s选自{480,576,688,864,1200}中的一个，选择的标准是使得缩放后的候选框大小与224×224最接近；<br><strong>3)特征提取</strong>：利用SPP-net网络结构提取特征；<br><strong>4)分类与回归</strong>：类似R-CNN，利用SVM基于上面的特征训练分类器模型，用边框回归来微调候选框的位置。</p><p>SPP-net解决了R-CNN区域提名时crop/warp带来的偏差问题，提出了SPP层，使得输入的候选框可大可小，但其他方面依然和R-CNN一样，因而依然存在不少问题，这就有了后面的Fast R-CNN。</p><h3 id="2-3-Fast-R-CNN"><a href="#2-3-Fast-R-CNN" class="headerlink" title="2.3 Fast R-CNN"></a>2.3 Fast R-CNN</h3><p>Fast R-CNN<a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="noopener">论文下载链接</a>是要解决R-CNN和SPP-net 2000个左右候选框带来的重复计算问题，其主要思想为：</p><p><strong>1)使用一个简化的SPP层</strong> —— RoI（Region of Interesting） Pooling层，操作与SPP类似；<br><strong>2)训练和测试是不再分多步</strong>：不再需要额外的硬盘来存储中间层的特征，梯度能够通过RoI Pooling层直接传播；此外，分类和回归用Multi-task的方式一起进行；<br><strong>3)SVD</strong>：使用SVD分解全连接层的参数矩阵，压缩为两个规模小很多的全连接层。</p><p>如下图所示，Fast R-CNN的主要步骤如下：</p><p><strong>1)特征提取</strong>：以整张图片为输入，利用CNN得到图片的特征层；<br><strong>2)区域提名</strong>：通过Selective Search等方法从原始图片提取区域候选框，并把这些候选框一一投影到最后的特征层；<br><strong>3)区域归一化</strong>：针对特征层上的每个区域候选框进行RoI Pooling操作，得到固定大小的特征表示；<br><strong>4)分类与回归</strong>：然后再通过两个全连接层，分别用softmax多分类做目标识别，用回归模型进行边框位置与大小微调。</p><p><img src="https://img.vim-cn.com/73/6182e894d1b1045e8f081ab04e9838f4fccc78.png" alt="图1.2.4: Fast R-CNN框架" title="Fast R-CNN框架"></p><p><center><strong><font face="黑体">图1.2.4: Fast R-CNN框架<font></font></font></strong></center></p><p>Fast R-CNN比R-CNN的训练速度（大模型L）快8.8倍，测试时间快213倍，比SPP-net训练速度快2.6倍，测试速度快10倍左右。</p><p><img src="https://img.vim-cn.com/14/547a8ffd8ad381d844ab5babb7705a15e5217c.png" alt="图1.2.5: Fast R-CNN, R-CNN, SPP-net的运行时间比较" title="Fast R-CNN, R-CNN, SPP-net的运行时间比较"></p><h3 id="2-4-Faster-R-CNN"><a href="#2-4-Faster-R-CNN" class="headerlink" title="2.4 Faster R-CNN"></a>2.4 Faster R-CNN</h3><p>Fast R-CNN使用Selective Search来进行区域提名，速度依然不够快。Faster R-CNN<a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">论文下载链接</a>则直接利用RPN（Region Proposal Networks)网络来计算候选框。RPN以一张任意大小的图片为输入，输出一批矩形区域提名，每个区域对应一个目标分数和位置信息。Faster R-CNN中的RPN结构如下图所示。</p><p><img src="https://img.vim-cn.com/a6/84c0dd210ef882bfcc4851447185a8b67661e1.png" alt="图1.2.6: Region Proposal Network(RPN)" title="Region Proposal Network(RPN)"></p><p>Faster R-CNN的主要步骤如下：</p><p><strong>1)特征提取：</strong>同Fast R-CNN，以整张图片为输入，利用CNN得到图片的特征层；<br><strong>2)区域提名：</strong>在最终的卷积特征层上利用k个不同的矩形框（Anchor Box）进行提名，k一般取9；<br><strong>3)分类与回归：</strong>对每个Anchor Box对应的区域进行object/non-object二分类，并用k个回归模型（各自对应不同的Anchor Box）微调候选框位置与大小，最后进行目标分类。</p><p>总之，Faster R-CNN弃用了Selective Search，引入了RPN网络，使得区域提名、分类、回归一起共用卷积特征，从而得到了进一步的加速。但是，Faster R-CNN需要对20000个Anchor Box先判断是否是目标（目标判定），然后再进行目标识别，分成了两步。</p><h3 id="2-5-R-FCN"><a href="#2-5-R-FCN" class="headerlink" title="2.5 R-FCN"></a>2.5 R-FCN</h3><p>前面的目标检测方法都可以细分为两个子网络：<br><strong>1)共享的全卷积网络；</strong><br><strong>2)不共享计算的ROI相关的子网络（比如全连接网络）。</strong></p><p>R-FCN<a href="https://arxiv.org/abs/1605.06409" target="_blank" rel="noopener">论文下载链接</a>则将最后的全连接层换为了一个位置敏感的的卷积网络，从而让所有计算都可以共享。具体来说，先把每个提名区域划分为k×k个网格，比如R-FCN原文中k的取值为3，则对应的九个网格分别表示：左上top-left，上中top-center，……，右下bottom-right，对应下图中的九宫格及不同颜色的块，每个Grid都有对应的编码，但预测时候会有C+1个输出，C表示类别数目，+1是因为有背景类别，全部的输出通道数量为k2×(C+1)。</p><p><img src="https://img.vim-cn.com/8e/e4bcbb5b551f18205e5fec149bc9c53561e415.png" alt="图1.2.7: R-FCN的person分类可视化过程" title="R-FCN的person分类可视化过程"></p><p><center><strong><font face="黑体">图1.2.7: R-FCN的person分类可视化过程<font></font></font></strong></center></p><p><img src="https://img.vim-cn.com/b8/f88a41b7e144c6af8cd5febcaa1ea45fc8f506.png" alt="图1.2.8: R-FCN框架" title="R-FCN框架"></p><p><center><strong><font face="黑体">图1.2.8: R-FCN框架<font></font></font></strong></center></p><p>需要注意的是，上面两张图中不同位置都存在一个九宫格，但是Pooling时候只有一个起作用，比如bottom-right层只有右下角的小块起作用。那么问题来了，这一层其他的8个框有什么作用呢？答案是它们可以作为其他ROI（偏左或偏上一些的ROI）的右下角。</p><p>R-FCN的步骤为：</p><p><strong>1)区域提名：</strong>使用RPN（Region Proposal Network，区域提名网络），RPN本身是全卷积网络结构；<br><strong>2)分类与回归：</strong>利用和RPN共享的特征进行分类。当做bbox回归时，则将C设置为4。</p><h2 id="3-端到端的方法"><a href="#3-端到端的方法" class="headerlink" title="3 端到端的方法"></a>3 端到端的方法</h2><p>接下来介绍端到端（End-to-End）的目标检测方法，这些方法无需区域提名，包括YOLO和SSD等</p><h3 id="3-1-YOLO"><a href="#3-1-YOLO" class="headerlink" title="3.1 YOLO"></a>3.1 YOLO</h3><p>YOLO<a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="noopener">论文下载链接</a>，即You Only Look Once，顾名思义就是只看一次，进一步把目标判定和目标识别合二为一，所以识别性能有了很大提升，达到每秒45帧，而在快速版YOLO(Fast YOLO，卷积层更少)中，可以达到每秒155帧。</p><p>网络的整体结构如下图所示，针对一张图片，YOLO的处理步骤为：</p><p><strong>1)把输入图片缩放到448×448大小；</strong><br><strong>2)运行卷积网络；</strong></p><p>对模型置信度卡阈值，得到目标位置与类别。<br><img src="https://img.vim-cn.com/82/a0b9eab552ef43125c5f1cb05626bb0e264982.png" alt="YOLO检测系统" title="YOLO检测系统"></p><p><center><strong><font face="黑体">YOLO检测系统<font></font></font></strong></center></p><p>网络的模型如下图所示，将448×448大小的图切成S×S的网格，目标中心点所在的格子负责该目标的相关检测，每个网格预测B个边框及其置信度，以及C种类别的概率。YOLO中S=7，B=2，C取决于数据集中物体类别数量，比如VOC数据集就是C=20。对VOC数据集来说，YOLO就是把图片统一缩放到448×448，然后每张图平均划分为7×7=49个小格子，每个格子预测2个矩形框及其置信度，以及20种类别的概率。<br><img src="https://img.vim-cn.com/7b/22d6072d723d5e3daeae00e55256114ea66a5c.png" alt="1.3.2: YOLO模型" title="1.3.2: YOLO模型"></p><p><center><strong><font face="黑体">1.3.2: YOLO模型<font></font></font></strong></center></p><p>YOLO简化了整个目标检测流程，速度的提升也很大，但是YOLO还是有不少可以改进的地方，比如S×S的网格就是一个比较启发式的策略，如果两个小目标同时落入一个格子中，模型也只能预测一个；另一个问题是Loss函数对不同大小的bbox未做区分。</p><h3 id="3-2-SSD"><a href="#3-2-SSD" class="headerlink" title="3.2 SSD"></a>3.2 SSD</h3><p>SSD<a href="">论文下载链接</a>，即Single Shot MultiBox Detector，冲着YOLO的缺点来的。SSD的框架如图1.3.3所示，图1.3.3(a)表示带有两个Ground Truth边框的输入图片，图1.3.3(b)和©分别表示8×8网格和4×4网格，显然前者适合检测小的目标，比如图片中的猫，后者适合检测大的目标，比如图片中的狗。在每个格子上有一系列固定大小的Box（有点类似前面提到的Anchor Box），这些在SSD称为Default Box，用来框定目标物体的位置，在训练的时候Ground Truth会赋予给某个固定的Box，比如图1.3.3(b)中的蓝框和图1.3.3©中的红框。<br><img src="https://img.vim-cn.com/7e/8628979292f8d7401f4d8b13387eaca70b0349.png" alt="图1.3.3: SSD框架" title="图1.3.3: SSD框架"></p><p><center><strong><font face="黑体">图1.3.3: SSD框架<font></font></font></strong></center></p><p>SSD的网络分为两部分，前面的是用于图像分类的标准网络（去掉了分类相关的层），后面的网络是用于检测的多尺度特征映射层，从而达到检测不同大小的目标。SSD和YOLO的网络结构对比如图所示。<br><img src="https://img.vim-cn.com/a5/1465c37aeed72fb972c968c3a4dd1737e979e8.png" alt="图1.3.4: SSD和YOLO的网络结构对比" title="图1.3.4: SSD和YOLO的网络结构对比"></p><p><center><strong><font face="黑体">图1.3.4: SSD和YOLO的网络结构对比<font></font></font></strong></center></p><p>SSD在保持YOLO高速的同时效果也提升很多，主要是借鉴了Faster R-CNN中的Anchor机制，同时使用了多尺度。但是从原理依然可以看出，Default Box的形状以及网格大小是事先固定的，那么对特定的图片小目标的提取会不够好。</p><h3 id="3-3-YOLOv2-amp-YOLO9000"><a href="#3-3-YOLOv2-amp-YOLO9000" class="headerlink" title="3.3 YOLOv2 &amp; YOLO9000"></a>3.3 YOLOv2 &amp; YOLO9000</h3><p>经过Joseph Redmon等的改进，YOLOv2和YOLO9000算法<a href="https://arxiv.org/abs/1612.08242" target="_blank" rel="noopener">论文下载链接</a>在2017年CVPR上被提出，并获得较佳论文提名，重点解决召回率和定位精度方面的误差。采用Darknet-19作为特征提取网络，增加了批量归一化（Batch Normalization）的预处理，并使用224×224和448×448两阶段训练ImageNet预训练模型后fine-tuning。</p><p>相比于原来的YOLO是利用全连接层直接预测bounding box的坐标，YOLOv2借鉴了Faster R-CNN的思想，引入anchor机制，利用K-Means聚类的方式在训练集中聚类计算出更好的anchor模板，在卷积层使用anchorboxes操作，增加候选框的预测，同时采用较强约束的定位方法，大大提高算法召回率。结合图像细粒度特征，将浅层特征与深层特征相连，有助于对小尺寸目标的检测。<br><img src="https://img.vim-cn.com/67/189843172337df022f9b8c2021d2820d561eb8.png" alt="图1.3.5: YOLOv2在VOC2007上的速度和精度" title="图1.3.5: YOLOv2在VOC2007上的速度和精度"></p><p><center><strong><font face="黑体">图1.3.5: YOLOv2在VOC2007上的速度和精度<font></font></font></strong></center></p><h3 id="3-4-R-SSD"><a href="#3-4-R-SSD" class="headerlink" title="3.4 R-SSD"></a>3.4 R-SSD</h3><p>2017年首尔大学提出了R-SSD算法，解决了SSD算法中不同层feature map都是独立作为分类网络的输入，容易出现相同物体被不同大小的框同时检测出来的情况，还有对小尺寸物体的检测效果比较差的情况。R-SSD算法一方面利用分类网络增加不同层之间的feature map联系，减少重复框的出现；另一方面增加feature pyramid中feature map的个数，使其可以检测更多的小尺寸物体。</p><p>特征融合方式采用同时利用pooling和deconvolution进行特征融合，这种特征融合方式使得融合后每一层的feature map个数都相同，因此可以共用部分参数，具体来讲就是default boxes的参数共享。<br><img src="https://img.vim-cn.com/51/2eb4f03b7237843c11c6289aeaa8a3c29caf3a.png" alt="图1.3.6: 三种特征融合方式" title="图1.3.6: 三种特征融合方式"></p><p><center><strong><font face="黑体">图1.3.6: 三种特征融合方式<font></font></font></strong></center></p><h3 id="3-5-DSSD"><a href="#3-5-DSSD" class="headerlink" title="3.5 DSSD"></a>3.5 DSSD</h3><p>为了解决SSD算法检测小目标困难的问题，2017年北卡大学教堂山分校的Cheng-Yang Fu等提出DSSD算法<a href="https://arxiv.org/abs/1701.06659" target="_blank" rel="noopener">论文下载链接</a>，将SSD算法基础网络从VGG-16更改为ResNet-101，增强网络特征提取能力，其次参考FPN算法思路利用去卷积结构将图像深层特征从高维空间传递出来，与浅层信息融合，联系不同层级之间的图像语义关系，设计预测模块结构，通过不同层级特征之间融合特征输出预测物体类别信息。</p><p>DSSD算法中有两个特殊的结构：Prediction模块；Deconvolution模块。前者利用提升每个子任务的表现来提高准确性，并且防止梯度直接流入ResNet主网络。后者则增加了三个Batch Normalization层和三个3×3卷积层，其中卷积层起到了缓冲的作用，防止梯度对主网络影响太剧烈，保证网络的稳定性。<br><img src="https://img.vim-cn.com/7d/7e2d38c22fcd1b45c7146f7e0bd1713498da50.png" alt="图1.3.7: SSD和DSSD网络结构对比" title="图1.3.7: SSD和DSSD网络结构对比"></p><p><center><strong><font face="黑体">图1.3.7: SSD和DSSD网络结构对比<font></font></font></strong></center></p><h3 id="3-6-DSOD"><a href="#3-6-DSOD" class="headerlink" title="3.6 DSOD"></a>3.6 DSOD</h3><p>2017年复旦大学提出DSOD算法，其并不是在mAP上和其他检测算法做比较，看谁的算法更有效或者速度更快，而是从另一个角度切入说明fine-tune和直接训练检测模型的差异其实是可以减小的，也就是说训练一个检测模型可以不需要大量的数据和预训练好的模型。</p><p>这是由于预训练模型的限制导致：迁移模型结构灵活性差，难以改变网络结构；分类任务预训练模型和检测任务训练会有学习偏差；虽然微调会减少不同目标类别分布的差异性，但深度图等特殊图像迁移效果差异较大。</p><p>SSD算法是在六个尺度的特征图上进行检测，将这六个检测结果综合起来，DSOD算法则则根据DenseNet的设计原理，将相邻的检测结果一半一半的结合起来。DSOD算法是基于SSD算法基础上做的修改，采用的特征提取网络是DenseNet。</p><p>采用Dense Block结构，能避免梯度消失的情况。同时利用Dense Prediction结构，也能大大减少模型的参数量，特征包含更多信息。设计stem结构能减少输入图片信息的丢失，stem结构由3×3卷积和2×2的max pool层组成，其还可以提高算法检测的mAP。<br><img src="https://img.vim-cn.com/d0/cd7bd9c0be552589831350f6b6fac0dadca594.png" alt="图1.3.8: DSOD预测层" title="图1.3.8: DSOD预测层"></p><p><center><strong><font face="黑体">图1.3.8: DSOD预测层<font></font></font></strong></center></p><h3 id="3-7-RON"><a href="#3-7-RON" class="headerlink" title="3.7 RON"></a>3.7 RON</h3><p>2017年清华大学提出了RON算法，结合two stage名的方法和one stage方法的优势，更加关注多尺度对象定位和负空间样本挖掘问题。</p><p>多尺度对象定位——各种尺度的物体可能出现在图像的任何位置，因此应考虑成千上万个具有不同位置/尺度/方位的区域。多尺度表征将显著改善各种尺度的物体检测，但是这些方法总是在网络的一层检测到各种尺度的对象；</p><p>负空间挖掘——对象和非对象样本之间的比例严重不平衡。因此，对象检测器应该具有有效的负挖掘策略。</p><p>RON算法通过设计方向连接结构，利用多尺度表征显著改善各种多尺度物体检测，同时为了减少对象搜索空间，在卷积特征图创建objectness prior引导目标对象搜索，训练时将检测器进行联合优化。并通过多任务损失函数联合优化了反向连接、objectness prior和对象检测，因此可直接预测各种特征图所有位置的最终检测结果。<br><img src="https://img.vim-cn.com/9c/1cc801288020ab016d174844d7cfd22bd3071b.png" alt="图1.3.8: RON" title="图1.3.8: RON"></p><p><center><strong><font face="黑体">图1.3.8: RON<font></font></font></strong></center></p><h3 id="3-8-YOLOv3"><a href="#3-8-YOLOv3" class="headerlink" title="3.8 YOLOv3"></a>3.8 YOLOv3</h3><p><img src="https://img.vim-cn.com/cd/4ea963310d4c5b6400e3ecd2bfd4aaa28648a6.png" alt="图1.3.9: 速度对比图" title="图1.3.9: 速度对比图"></p><p><center><strong><font face="黑体">图1.3.9: 速度对比图<font></font></font></strong></center></p><p>从上边的图中就可以看出，YOLOv3<a href="https://arxiv.org/abs/1804.02767" target="_blank" rel="noopener">论文下载链接</a>最主要的提升网络的就算速度。YOLOv3能在22毫秒内完成处理，并取得28.2mAP的成绩。它的精度和SSD相当，但速度要快上3倍。但是整体模型也变得复杂不少。</p><h2 id="4-小结"><a href="#4-小结" class="headerlink" title="4 小结"></a>4 小结</h2><p>随着深度学习技术在图像各领域的研究深入，出现越来越多的新理论、新方法。two stage的方法和基于回归思想的one stage方法两者相互借鉴，不断融合，取得了很好的效果，也为我们展示了一些未来发展趋势：</p><p><strong>1)参考上下文特征的多特征融合；</strong><br><strong>2)多尺度的对象定位；</strong><br><strong>3)结合循环神经网络（RNN）的图像语义分析。</strong><br><img src="https://img.vim-cn.com/5f/a81a946ed200685240dc5c7771883f2e7eca39.png" alt="图2.1:部分目标检测算法精度和速度对比" title="图2.1:部分目标检测算法精度和速度对比"></p><p><center><strong><font face="黑体">图2.1:部分目标检测算法精度和速度对比<font></font></font></strong></center></p><p><strong>诚然，目标检测还有很长的路要走，比如业界公认较难的小目标检测问题。但我们有理由相信会不断有各种突破出现，还是那句老话——科学技术是第一生产力，期待未来基于深度学习的目标检测的进一步突破！</strong></p><hr><p>作者：StrongerTang</p><p>来源：CSDN</p><p>原文：<a href="https://blog.csdn.net/qq_41590635/article/details/89372866" target="_blank" rel="noopener">https://blog.csdn.net/qq_41590635/article/details/89372866</a></p></center>]]></content>
      
      
      <categories>
          
          <category> 科普文章 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【转载】浅析自动驾驶技术</title>
      <link href="/main/2018/12/30/example/2/"/>
      <url>/main/2018/12/30/example/2/</url>
      
        <content type="html"><![CDATA[<p>少年时代，一部来自美国的电视剧《霹雳游侠》让我产生了一个不切实际的愿望——拥有一<br>部同片中KITT一样，能“说话”、可以自动驾驶的帅车。时光荏苒，路上的私家车渐渐多起来，它们之中也有装配能与人对话的语音操作系统了，但是那些自动驾驶的内容似乎还停留在我的梦中，事实真的如此吗？</p><a id="more"></a><h2 id="自动驾驶到底是什么？"><a href="#自动驾驶到底是什么？" class="headerlink" title="自动驾驶到底是什么？"></a>自动驾驶到底是什么？</h2><p>　　在影视作品中，搭配自动驾驶功能的车大多外形前卫，里面或许还有不少闪烁着各种图像以及天书一样难懂内容的显示屏，它们可以很贴心的“揣测”到车主的心思，救车主于水火之中。好了，让我们停止开脑洞，看看现实中这些自动驾驶系统吧。</p><p><img src="https://img.vim-cn.com/ce/7fb76085efb68a46fa22a132f20505c7ecaafb.webp" alt=""></p><p>　　我们想象中的自动驾驶其实称作无人驾驶即Autonomous vehicles或Self-piloting automobile更为恰当，它是电脑控制车辆行驶的最高级别，其研发时间最早可以追溯至上世纪中叶。如同其他很多事物一样，自动驾驶实际上也有一个技术循序渐进发展的过程。或者说，自动驾驶也需分为不同阶段。</p><p><img src="https://img.vim-cn.com/b5/e1e301237dfabb93fce0df1ec9c1a676042da4.webp" alt=""><br><img src="https://img.vim-cn.com/08/9b8918ef9ebc2b80a5f5ef988954505272006f.webp" alt=""></p><p>　　目前，大部分的科学家、学者以及车企将自动驾驶分为三个阶段，即辅助驾驶阶段、半自动驾驶阶段、全自动驾驶阶段。换句话说，如果您家的爱车配备了诸如自适应巡航、车道保持等功能的话，它就可以被称为是辆具备部分自动驾驶功能的汽车了。</p><p><img src="https://img.vim-cn.com/f0/c2b6bf71716168c971f3742c4d4cf8c3fa84e5.webp" alt=""></p><p>　　影响现阶段自动驾驶技术普及的因素有很多，大体上可以总结为下图所示的6点。其中那些历经多年“考验”的法律法规一方面保护着人们的安全，另一方面，类似1968年维也纳驾驶公约中标注的“必须保证驾驶员全时操控车辆”这样的规则也限制了新技术的推广。</p><p><img src="https://img.vim-cn.com/61/976f04bd540db7f7b36fb1e62f70bc4472920c.webp" alt=""></p><p>　　细心的朋友可能已经发现了，在上面的表格中，有一项社会、公众关注度是阻碍技术普及的因素，对此，相关机构对于德国与中国民众进行了抽样调查，其中反映出的结果与您设想的可能存在不小的区别。</p><p><img src="https://img.vim-cn.com/c7/4fa1e0f5ca273842ff5e0e13369a5d91d49587.webp" alt=""><br><img src="https://img.vim-cn.com/8a/19f842c4e54b6b840d039d07cc8f19b5397b41.webp" alt=""></p><p>　　通过数据我们意外发现，在自动驾驶技术接受程度上，中国民众接受程度远超德国民众，此外，德国民众随着年龄的增长，逐渐增高的不接受比例这种情况也没有出现在中国消费者心中，这显示出了我国民众对于新技术的渴求。</p><h2 id="市面上自动驾驶现在处于何种阶段？"><a href="#市面上自动驾驶现在处于何种阶段？" class="headerlink" title="市面上自动驾驶现在处于何种阶段？"></a>市面上自动驾驶现在处于何种阶段？</h2><p>　　目前市面上大多数品牌的辅助驾驶系统实际上就是自动驾驶的初级阶段，它们大都可以为车主提供重要或有益的驾驶相关信息或提高行车安全，我们看看其中比较有代表性的三个品牌各自发展方向。</p><h3 id="沃尔沃自动驾驶"><a href="#沃尔沃自动驾驶" class="headerlink" title="沃尔沃自动驾驶"></a>沃尔沃自动驾驶</h3><p>　　作为一向将安全作为自己品牌基调的沃尔沃，其很早便在旗下车型上配备了辅助驾驶系统，City Safety这套系统的名字也随着沃尔沃的宣传深入人心。</p><p><img src="https://img.vim-cn.com/3b/89ff3b72c29843e7d7d3a3bdacf58035632e9b.webp" alt=""><br><img src="https://img.vim-cn.com/d2/82b05a0e3c8e7e90d2adde5ba5efb19bafe35d.webp" alt=""><br><img src="https://img.vim-cn.com/59/60ab0a89a8a21b20f6eb32dcb24ec1277d812a.webp" alt=""><br><img src="https://img.vim-cn.com/d8/e21199e4c4b70091f7edb9bd65d05bc443a8fc.webp" alt=""><br><img src="https://img.vim-cn.com/c8/7b72df1c7e87ed6ffdb58814b46c2c317288d3.webp" alt=""><br><img src="https://img.vim-cn.com/8a/726c4476957c9df4b2ca520c1a81b0cc4f6067.webp" alt=""></p><p>　　沃尔沃这个向来标榜安全性的品牌在主动安全技术上一直处于领先地位，这正是自动驾驶技术初级阶段配备的技术，这也令我们更加期待其公布的自动驾驶技术。那么，除了它以外，不同“调性”的品牌自动驾驶技术研发的成果如何呢？</p><h3 id="宝马自动驾驶"><a href="#宝马自动驾驶" class="headerlink" title="宝马自动驾驶"></a>宝马自动驾驶</h3><p>　　与沃尔沃不同，宝马品牌的性格特点与操控二字始终脱离不了关系，正如前人说过的，跑的快不难，难的是停下来，在自动驾驶领域，宝马一直也没闲着。</p><p><img src="https://img.vim-cn.com/8c/0279148341a8d6f45565c8848042638e2ca06b.webp" alt=""><br><img src="https://img.vim-cn.com/45/1a829accf366eecbdcf27ba66b923ebb309d49.webp" alt=""><br><img src="https://img.vim-cn.com/ea/3fccab91cb2c8429fa1ac278d1fe4545527755.webp" alt=""><br><img src="https://img.vim-cn.com/59/caa9475b3596ad1f739415d9b3b07827f21ad7.webp" alt=""><br><img src="https://img.vim-cn.com/6b/15935ebede9ab6d0aab249da570b3ce999dd06.webp" alt=""></p><h3 id="丰田"><a href="#丰田" class="headerlink" title="丰田"></a>丰田</h3><p>　　作为亚洲车企，丰田品牌的基调似乎与欧洲、美国车企都不同，追求均衡、中庸是此前丰田留给人的印象，在自动驾驶方面，其走出的路也与前面提到的两个品牌截然不同。</p><p>　　首先，2014年，丰田在日本本土公布了其研发的主动安全技术套装，这套名为TSA（Toyota Safety Sense）的系统包含了全新全景模式影像监测系统、自适应远光灯控制系统、第二代智能停车辅助系统等诸多主动安全技术。　</p><p><img src="https://img.vim-cn.com/80/06d73dcf8f5f8bbbb3307ad40adcade8b138d1.webp" alt=""></p><p>　　除了这部分已经公开即将搭载新车型的技术，丰田还有一些秘密武器，这就是名为ITS（Intelligent Transport System）的智能交通系统。它的特殊之处在于通过地面设备、车辆对其他车辆和行人的探测和信号通信，对驾驶者进行有效提示，从而实现预防碰撞，提升交通安全性的目的。</p><p><img src="https://img.vim-cn.com/6a/43880d81de8e12560f90ecd4de36cb13944ab9.webp" alt=""><br><img src="https://img.vim-cn.com/4c/bfeadefacb4f17f91aff3460574407d12c3b30.webp" alt=""></p><h2 id="日产自动紧急避让系统"><a href="#日产自动紧急避让系统" class="headerlink" title="日产自动紧急避让系统"></a>日产自动紧急避让系统</h2><p>　　品完了上面的“各式大餐”后，让我们来个“饭后甜点”吧。相比那些功能多样的自动驾驶系统，日产这套名为自动紧急避让的系统似乎更实用。它可以有效应对城市中愈发复杂的交通状况，当有行人突然进入车道后，系统可以自行判断情况并操作进行避让，只是如果碰到单向行车道的情况，这套系统如何应对，有待观察。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>　　如同其他很多新鲜事物诞生、推广、普及的过程一样，自动驾驶也正在经历这样的阵痛期。在技术上，核心部件的运算能力以及成本过高并不是阻碍其推广的最大问题，传统关键的束缚、法律法规的约束、行业间的利益关系等等方面均制约着它的推广。自动驾驶真正走进我们的生活还有很长的路要走。</p><hr><p>作者：智享汽车圈</p><p>原文：<a href="https://mp.weixin.qq.com/s/yCHBY2W-_1jEMPU9kgDwyA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/yCHBY2W-_1jEMPU9kgDwyA</a></p>]]></content>
      
      
      <categories>
          
          <category> 科普文章 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>【转载】自动驾驶 vs 机器人环境感知</title>
      <link href="/main/2018/12/30/example/3/"/>
      <url>/main/2018/12/30/example/3/</url>
      
        <content type="html"><![CDATA[<p><img src="https://img.vim-cn.com/38/34e2aa149e195943174c902ac2b86e2b5ed911.webp" alt=""></p><a id="more"></a><p>自动驾驶和机器人共同的三大关键技术为：</p><p><img src="https://img.vim-cn.com/c2/de3a2b6a2817c9446edc255f7558817651cc60.webp" alt=""></p><p>环境感知是其中最重要、最基础的一环。</p><p><img src="https://img.vim-cn.com/bd/1863696dedb0d3d0de6cb9f31e78483d597f81.webp" alt=""></p><p>自动驾驶和机器人主要通过传感器来获取周围环境信息，同时也会通过高精度地图和IoT技术来扩展环境感知能力。</p><p><img src="https://img.vim-cn.com/a3/d30c619cb7714aa1c8a014ea2ede1657797a28.webp" alt=""></p><p>下面我们来了解一下每类传感器的特性，以及在机器人和自动驾驶汽车中的使用差异。</p><h2 id="一、-摄像头"><a href="#一、-摄像头" class="headerlink" title="一、 摄像头"></a>一、 摄像头</h2><p>摄像头是机器人或自动驾驶汽车的眼睛，分类如下：</p><p><img src="https://img.vim-cn.com/d2/78fdd728f56ccb7d8fee1324955d9ca1de29a8.webp" alt=""></p><h3 id="1-普通单目摄像头"><a href="#1-普通单目摄像头" class="headerlink" title="1. 普通单目摄像头"></a>1. 普通单目摄像头</h3><p>通过图像匹配进行目标识别，再通过目标在图像中的大小去估算目标距离，准确识别是准确估算距离的第一步。</p><p><img src="https://img.vim-cn.com/73/48114d3e30352b5afa0f3cf8698455a8e5cb33.webp" alt=""><br><img src="https://img.vim-cn.com/78/17370f5c03a6c4edc4081fb5bcda02ab38148f.webp" alt=""></p><h3 id="2-单目结构光深度摄像头"><a href="#2-单目结构光深度摄像头" class="headerlink" title="2. 单目结构光深度摄像头"></a>2. 单目结构光深度摄像头</h3><p>由一个RGB摄像头、结构光投射器(红外)和结构光深度感应器(CMOS)组成，通过投影一个预先设计好的图案作为参考图像(编码光源)，将结构光投射至物体表面，再通过深度感应器接收该物体表面反射的结构光图案。</p><p>这样，同样获得了两幅图像，一幅是预先设计的参考图像，另一幅是相机获取的物体表面反射的结构光图案。</p><p>由于接收图案会因物体的立体形状而发生变形，因此可以通过该图案在摄像机上的位置和形变程度来计算物体表面的空间信息。<br><img src="https://img.vim-cn.com/8d/74296b0f8f6cf67cebe210c52b5f320e2a33c6.webp" alt="单目结构光 Kinect一代" title="单目结构光 Kinect一代"></p><center><font face="黑体">单目结构光 Kinect一代<font></font></font></center><p>同样是进行图像匹配，这种方法与双目匹配比较好处在于，参考图像不是获取的，而是经过专门设计的图案，因此特征点是已知的，而且更容易从测试图像中提取。</p><h3 id="3-双目深度摄像头"><a href="#3-双目深度摄像头" class="headerlink" title="3. 双目深度摄像头"></a>3. 双目深度摄像头</h3><p>双目摄像头的测距方式则是通过对图像视差进行计算，直接对前方景物进行距离测量。双目摄像头的原理与人眼相似，人眼能够感知物体的远近，是由于两只眼睛对同一个物体呈现的图像存在差异，也称“视差”。</p><p>物体距离越远，视差越小，反之视差越大。视差的大小对应着物体与眼睛之间距离的远近，这也是3D电影能够使人有立体层次感知的原因。</p><p><img src="https://img.vim-cn.com/d6/48c179142d29637de530b8febdda2e26c8f5d6.webp" alt=""></p><p><strong>优点</strong></p><p>1） 双目系统成本比单目系统要高，但尚处于可接受范围内，并且与激光雷达等方案相比成本较低。</p><p>2） 没有识别率的限制，因为从原理上无需先进行识别再进行测算，而是对所有障碍物直接进行测量。</p><p>3） 精度比单目高，直接利用视差计算距离。</p><p>4） 无需维护样本数据库，因为双目没有样本的概念。</p><p><strong>难点</strong></p><p>1） 计算量大，对计算单元的性能要求非常高，这使得双目系统的产品化、小型化的难度较大。</p><p>2） 匹配，双目匹配采用三角测量原理，完全基于图像处理技术，通过寻找两个图像中相同的特征点得到匹配点，从而得到深度值。</p><p>双目测距中光源是环境光或者白光这种没有经过编码的光源，图像识别完全取决于被拍摄的物体本身的特征点，对表面颜色和纹理特征不明显的物体失效，匹配的精度和正确性很难保证，因此出现了结构光技术来解决匹配问题。</p><p>因为结构光光源带有很多特征点或者编码，因此提供了很多的匹配角点或者直接的码字，可以很方便的进行特征点的匹配。</p><h3 id="4-TOF深度摄像头"><a href="#4-TOF深度摄像头" class="headerlink" title="4. TOF深度摄像头"></a>4. TOF深度摄像头</h3><p>TOF是Time of flight的简写，直译为飞行时间的意思。</p><p>所谓飞行时间法3D成像，是通过给目标连续发送光脉冲，然后用传感器接收从物体返回的光，通过探测光脉冲的飞行（往返）时间来得到目标物体的距离。这种技术跟3D激光传感器原理基本类似，只不过3D激光传感器是逐点扫描，而TOF相机则是同时得到整幅图像的深度信息。</p><p>TOF相机与普通机器视觉成像过程也有类似之处，都是由光源、光学部件、传感器、控制电路以及处理电路等几部单元组成。<br><img src="https://img.vim-cn.com/5f/fb40477fc86661cf5706f81c6cfcd2d5340e6b.webp" alt="单目TOF Kinect二代" title="单目TOF Kinect二代"></p><center><font face="黑体">单目TOF Kinect二代<font></font></font></center><p>各种摄像头性能和成本比较：</p><p><img src="https://img.vim-cn.com/26/9425b64009c0bea9784f01f979a83a2877a3bb.webp" alt=""></p><p>由上表对比可知，无论是结构光还是TOF方案，都需要增加主动光，因此，其检测距离受到了光强度的限制，无法适用于远距离的检测，一般只用于机器人的感知，而普通单目和双目摄像头除了在机器人上应用，还可以用于ADAS和自动驾驶汽车上。</p><h2 id="二、激光雷达"><a href="#二、激光雷达" class="headerlink" title="二、激光雷达"></a>二、激光雷达</h2><p>激光雷达以激光作为信号源，由激光器发射出的脉冲激光，打到对面物体上，引起散射，一部分光波会反射到激光雷达的接收器上，根据激光测距原理计算，就得到从激光雷达到目标点的距离，脉冲激光不断地扫描目标物，就可以得到目标物上全部目标点的数据，用此数据进行成像处理后，即可得到精确的目标物体图像。</p><p><img src="https://img.vim-cn.com/56/bb48cd9e427b8fdcf353e0c09d7729de3f057a.webp" alt=""></p><p>激光雷达分为单线和多线，常见的多线激光雷达有4线，8线，16线，32线和64线。</p><ul><li>单线激光雷达扫描的区域可以简单定义为一个平面，是一个二维扫描方案</li><li>4线、8线雷达纵向扫描范围从3.2°到6.4°，这个范围不能称为一个3D的扫描，一般定义为2.5D扫描方案。</li><li>64线雷达扫描的整个范围面就比较大，纵向甚至可以一直到30多度，讲究对整个环境、3D的点云数据收集，单位时间内收集到的反馈点数多，数据量大。<br><img src="https://img.vim-cn.com/33/5c74db724f2b5946ba862c7e371b9ad6bdaf00.webp" alt="SICK单线激光雷达" title="SICK单线激光雷达"><center><font face="黑体">SICK单线激光雷达<font></font></font></center></li></ul><p><img src="https://img.vim-cn.com/c3/604e969c08a8eb012e5a636e0be46f85e38376.webp" alt="单线二维激光雷达扫描图" title="单线二维激光雷达扫描图"></p><center><font face="黑体">单线二维激光雷达扫描图<font></font></font></center><p><img src="https://img.vim-cn.com/aa/edbf6b0e5b051601f045ee109ac6bbf4f33b1a.webp" alt="Velodyne多线激光雷达" title="Velodyne多线激光雷达"></p><center><font face="黑体">Velodyne多线激光雷达<font></font></font></center><p><img src="https://img.vim-cn.com/7a/7c1d0bb7e0d6a5064ac793745f740ed6c7a544.webp" alt="64线三维激光雷达扫描图" title="64线三维激光雷达扫描图"></p><center><font face="黑体">64线三维激光雷达扫描图<font></font></font></center><p>激光雷达普遍用于定位、障碍物检测、物体分类、动态物体跟踪等应用，在机器人和自动驾驶汽车上都有使用。</p><ul><li>由于机器人的工作环境相对来说比较简单，而且迫于成本压力，一般采用单线激光雷达，用于定位和检测周边障碍物。</li><li>自动驾驶汽车一般采用32线或64线的三维激光雷达置于车顶，完成对车辆四周较远物体的检测分类和跟踪。另外，在车灯或者保险杠附近的位置还需要安装4线和8线激光雷达，主要对车顶三维激光雷达进行补盲，对近距离的车辆、行人以及地线、马路牙、路肩、路栏等进行识别。</li></ul><p><strong>缺点</strong></p><p>激光雷达容易受到大气条件以及工作环境的烟尘的影响，要实现全天候的工作环境是非常困难的事情。</p><h2 id="三、毫米波雷达"><a href="#三、毫米波雷达" class="headerlink" title="三、毫米波雷达"></a>三、毫米波雷达</h2><p>毫米波是指波长在 1-10mm 之间的电磁波,换算成频率后,毫米波的频率位于30-300GHz 之间。</p><p>毫米波的波长介于厘米波和光波之间,因此毫米波兼有微波制导和光电制导的优点:</p><ol><li><p>同厘米波导引头相比, 毫米波导引头具有体积小、质量轻和空间分辨率高的特点。</p></li><li><p>与红外、激光等光学导引头相比, 毫米波导引头穿透雾、烟、灰尘的能力强,传输距离远,具有全天候全天时的特点，在雨天、大雪天气下，毫米波雷达是非常不错的选择。</p></li><li><p>性能稳定,不受目标物体形状、颜色等干扰。 毫米波雷达很好的弥补了如红外、激光、超声波、摄像头等其它传感器在车载应用中所不具备的使用场景。</p></li></ol><p>目前车载雷达的频率主要分为24GHz频段和77GHz频段，其中77GHz频段代表着未来的趋势，这是国际电信联盟专门划分给车用雷达的频段。</p><p>严格来说77GHz的雷达才属于毫米波雷达，但是实际上24GHz的雷达也被称为毫米波雷达。</p><p><img src="https://img.vim-cn.com/2a/e41823c07a12fd6022466eb44a91275c918da3.webp" alt=""></p><p>毫米波雷达在测量目标的距离、速度和角度上展现的性能和其它传感器还是略有区别的。</p><ul><li>视觉传感器得到的是二维信息，没有深度信息，而毫米波雷达则是具备深度信息的，可以提供目标的距离。</li><li>激光雷达对于速度并不敏感，而毫米波雷达则对速度非常敏感，可以直接获得目标的速度，因为毫米波雷达会有很明显的多普勒效应，通过检测其多普勒频移可将目标的速度提取出来。</li><li>毫米波雷达最基本的探测技术是使用FMCW连续线性调频波去探测前方物体的距离，毫米波雷达发射的是连续波，在后端处理上要比激光雷达的运算量大。</li></ul><p>毫米波雷达在ADAS领域是很难被取代的传感器，虽然有一些缺点，但是是唯一的全天候工作的传感器。</p><p>其测速、测距的精度要远高于视觉，与激光雷达相比，其测速精度会高一些，穿透力会更好。</p><p>而对于机器人的应用场景，利用毫米波雷达来探测障碍物，显得有点奢侈了，一般采用更低成本的超声波雷达来替代，但对于一些特殊应用场景的机器人（譬如消防，大型物流），由于需要在复杂环境下支持全天候、全天时作业，就必须采用毫米波雷达来实现避障。</p><h2 id="四、超声波雷达"><a href="#四、超声波雷达" class="headerlink" title="四、超声波雷达"></a>四、超声波雷达</h2><p>超声波雷达是利用传感器内的超声波发生器产生 40KHz的超声波,再由接收探头接收经障碍物反射回来的超声波,根据超声波反射接收的时间差计算与障碍物之间的距离。</p><p>超声波雷达成本较低,探测距离近，精度高,且不受光线条件的影响,因此常用于泊车系统中。</p><p><img src="https://img.vim-cn.com/53/0e979bd6c00b0ce31add586e84abea766281dd.webp" alt=""></p><p>超声波最大的缺点就是检测角度太小，一辆车需要在不同角度安装好几个，除此以外，都比上面几种方案更好。</p><p><strong>优点</strong></p><ul><li>防水，防尘，少量的泥沙遮挡也无妨</li><li>有金属材质的探头，可以与车体外壳结合的很好</li><li>通常适合3m内的检测，由于其空气损耗大，检测角度又小，因此车辆之间的干扰较小</li><li>最小的监测距离可达到0.1-0.3m</li><li>成本不高</li></ul><p>对于较常见的40KHz超声波传感器，其测距精度大约是1~3cm左右（取决于后端电路和数据处理性能），这个范围也能满足倒车雷达的要求，所以在倒车雷达的各个方案中，超声波是最容易被用户接受的。</p><p>另外，超声波雷达由于成本低，检测距离适中，因此在机器人的避障中应用也很广。汽车相对于绝大部分室内应用的机器人来说，对防护等级要求较高，因此，汽车上用的都是高防护等级的收发一体化的超声波雷达。</p><h2 id="五、红外"><a href="#五、红外" class="headerlink" title="五、红外"></a>五、红外</h2><p>红外线的工作原理是利用高频调制的红外线在待测距离上往返产生的相位移推算出光束度越时间△t，从而根据D＝C△t/2得到距离D。红外传感器的测距基本原理为发光管发出红外光，光敏接收管接收前方物体反射光，据此判断前方是否有障碍物。根据发射光的强弱可以判断物体的距离，它的原理是接收管接收的光强随反射物体的距离而变化的，距离近则反射光强，距离远则反射光弱。 </p><p>目前，使用较多的一种传感器红外光电开关，它的发射频率一般为38 kHz左右，探测距离一般比较短，通常被用作近距离障碍目标的识别。红外只适合短距离测距，因此基本上只用于低速移动的机器人上防碰撞。</p><p><img src="https://img.vim-cn.com/fa/9b4133f7c6f1207afa189af0389296ffab1895.png" alt=""></p><p>各种传感器的技术指标对比：</p><p><img src="https://img.vim-cn.com/d2/651b2246fc872a3e4834b374452a5a2748b07f.webp" alt=""></p><h2 id="六、IMU"><a href="#六、IMU" class="headerlink" title="六、IMU"></a>六、IMU</h2><p>IMU（惯性测量单元）是测量物体三轴姿态角(或角速率)以及加速度的装置。一般一个IMU包含三个单轴的加速度计和三个单轴的陀螺，加速度计检测物体在载体坐标系统独立三轴的加速度信号，而陀螺检测载体相对于导航坐标系的角速度信号，测量物体在三维空间中的角速度和加速度，并以此计算出物体的姿态。</p><p><img src="https://img.vim-cn.com/68/2c5d5ea14d450d813edfd2600de7c73ccc3d32.jpg" alt=""></p><p>惯性传感器的定位误差会随着运行时间增长，但由于其是高频传感器，在短时间内可以提供稳定的实时位置更新。IMU大多用在需要进行运动控制的设备，如汽车和机器人上，在导航中有着很重要的应用价值。</p><h2 id="七、GPS"><a href="#七、GPS" class="headerlink" title="七、GPS"></a>七、GPS</h2><p>GPS由GPS接收机和卫星天线组成，主要通过卫星来计算我们当前的位置和速度。通过测量从卫星上接收信号的时间，设备能够实现精确度大约在10m左右的定位。</p><p>一个略为改进过的定位系统上线了，我们可以把它称为差分全球定位系统(DGPS)，因为使用了地面基准站的关系，这个系统的精确度被提升到了1m。但这个精度对机器人和自动驾驶汽车的使用场景来说，都是不够的。因此，GPS一般需要融合IMU实现厘米级的定位。</p><p>另外，由于室内无法接收到GPS信号，因此，对于室内机器人则无法使用GPS进行定位</p><h2 id="八、信息交互"><a href="#八、信息交互" class="headerlink" title="八、信息交互"></a>八、信息交互</h2><p>对于自动驾驶汽车来说，还有一类技术虽然不是主动式的探测元件，但是属于协同式的全局数据辅助，可以扩展智能车的环境感知能力，在感知层同样扮演着不可或缺的角色，包括高精度地图、V2X车联网技术。每种类型的感知技术都有自己的优势和弊端，它们相互补充融合，最终使智能车达到驾驶场景下非常高的安全性要求。</p><h2 id="九、结语"><a href="#九、结语" class="headerlink" title="九、结语"></a>九、结语</h2><p>传感器是机器人和自动驾驶汽车环境感知的基础，对于各个传感器采集的数据还需要算法来处理，这样才能进行自身的定位和环境障碍的识别，因此，单个传感器数据的处理以及多传感器数据融合的算法非常关键，后续我们将进行算法部分的详细介绍。</p><hr><p>作者：智享汽车圈</p><p>原文：<a href="https://mp.weixin.qq.com/s/px5z_q5bfwT8_R4_O9aSUg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/px5z_q5bfwT8_R4_O9aSUg</a></p>]]></content>
      
      
      <categories>
          
          <category> 科普文章 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
